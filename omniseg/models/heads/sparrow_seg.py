"""
SparrowSegHead: A Custom SOTA Model for Fast Instance Segmentation.

This module adapts the Sparrow-Seg architecture to the provided LW-DETR-style
template. It combines a state-of-the-art backbone and feature neck with a novel
dynamic query mechanism for high-speed, high-performance instance segmentation.

Key Architectural Features of this Implementation:
1.  **Dynamic Query Generation**: Instead of fixed, learnable object queries (like
    in DETR), this model generates queries dynamically from the image's high-level
    semantic features. This makes the model more adaptive and efficient.

2.  **Lightweight Deformable Decoder**: Utilizes a very shallow (2-layer)
    transformer decoder with deformable attention. This drastically reduces
    computational cost compared to standard 6-layer DETR decoders, enabling
    faster inference.

3.  **Advanced Feature Fusion (PANet)**: Leverages the provided Path Aggregation
    Network (PANet) neck to ensure rich, multi-scale features are available
    for both query generation and mask refinement.

4.  **Complete Instance Segmentation Output**: Produces class logits, bounding
    boxes, and pixel-wise masks in a single forward pass, fitting seamlessly
    into the provided training and inference structure.

Install requirements:
    pip install timm scipy torch torchvision
"""

from copy import deepcopy
from collections import OrderedDict
from typing import Dict, List, Optional

import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy.optimize import linear_sum_assignment
from torchvision.ops import FeaturePyramidNetwork, box_convert, generalized_box_iou
from ..base import BaseHead
from ..backbones import get_backbone

# --- Deformable attention (from your template) ---
try:
    from torchvision.ops.deformable_attention import DeformableAttention
    print("INFO: Successfully imported torchvision DeformableAttention.")
except ImportError:
    print("=" * 60)
    print("CRITICAL WARNING: Failed to import the official DeformableAttention.")
    print("Falling back to a placeholder. This will be slow and may have issues.")
    print("Please check your torch/torchvision/CUDA installation.")
    print("=" * 60)
    class PurePyTorchDeformableAttention(nn.Module):
        def __init__(self, d_model, n_levels, n_heads, n_points):
            super().__init__()
            self.d_model = d_model
            # This is a non-functional placeholder for syntax compatibility
        def forward(self, query, reference_points, input_flatten, input_spatial_shapes, input_level_start_index, input_padding_mask=None):
            return query
    DeformableAttention = PurePyTorchDeformableAttention

# --- Helpers (from your template) ---
def _get_clones(module: nn.Module, N: int) -> nn.ModuleList:
    return nn.ModuleList([deepcopy(module) for _ in range(N)])

# --- Backbone wrapper (from your template, uses PANet) ---
class PANetFPN(nn.Module):
    def __init__(self, in_channels_list: list, out_channels: int):
        super().__init__()
        self.fpn = FeaturePyramidNetwork(in_channels_list=in_channels_list, out_channels=out_channels)
        num_panet_blocks = len(in_channels_list) - 1
        self.panet_convs_down = nn.ModuleList([
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1) for _ in range(num_panet_blocks)
        ])
        self.output_convs = nn.ModuleList([
            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1) for _ in range(len(in_channels_list))
        ])

    def forward(self, features_dict: OrderedDict) -> OrderedDict:
        fpn_outputs = self.fpn(features_dict)
        fpn_keys = sorted(features_dict.keys())
        fpn_output_list = [fpn_outputs[k] for k in fpn_keys]
        panet_outputs = [fpn_output_list[0]]
        for i in range(len(fpn_output_list) - 1):
            downsampled_prev = self.panet_convs_down[i](panet_outputs[-1])
            fused_feature = fpn_output_list[i + 1] + downsampled_prev
            panet_outputs.append(fused_feature)
        final_outputs = OrderedDict()
        for i, (k, feat) in enumerate(zip(fpn_keys, panet_outputs)):
            final_outputs[k] = self.output_convs[i](feat)
        return final_outputs

class GenericBackboneWithFPN(nn.Module):
    def __init__(self, backbone_type: str, fpn_out_channels: int, dummy_input_size: int):
        super().__init__()
        self.backbone = get_backbone(backbone_type)
        self.out_channels = fpn_out_channels
        with torch.no_grad():
            dummy_input = torch.randn(1, 3, dummy_input_size, dummy_input_size)
            feat_dict = self.backbone(dummy_input)
        self._feature_keys = list(feat_dict.keys())
        in_channels_list = [feat_dict[k].shape[1] for k in self._feature_keys]
        self.fpn = PANetFPN(in_channels_list=in_channels_list, out_channels=self.out_channels)

    @property
    def num_feature_levels(self) -> int:
        return len(self._feature_keys)

    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:
        features = self.backbone(x)
        ordered_features = OrderedDict((k, features[k]) for k in self._feature_keys)
        return self.fpn(ordered_features)

# --- Decoder layer (from your template) ---
class DeformableTransformerDecoderLayer(nn.Module):
    def __init__(self, d_model: int = 256, n_heads: int = 8, d_ffn: int = 1024, dropout: float = 0.1, n_levels: int = 4, n_points: int = 4):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.cross_attn = DeformableAttention(d_model, n_levels, n_heads, n_points)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout2 = nn.Dropout(dropout)
        self.linear1 = nn.Linear(d_model, d_ffn)
        self.activation = nn.ReLU()
        self.dropout3 = nn.Dropout(dropout)
        self.linear2 = nn.Linear(d_ffn, d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout4 = nn.Dropout(dropout)

    def forward(self, tgt, memory, ref_points_c=None, memory_spatial_shapes=None, level_start_index=None, **kwargs):
        tgt2 = self.self_attn(tgt, tgt, tgt)[0]
        tgt = self.norm1(tgt + self.dropout1(tgt2))
        tgt2 = self.cross_attn(query=tgt, reference_points=ref_points_c, input_flatten=memory, input_spatial_shapes=memory_spatial_shapes, input_level_start_index=level_start_index)
        tgt = self.norm2(tgt + self.dropout2(tgt2))
        tgt2 = self.linear2(self.dropout3(self.activation(self.linear1(tgt))))
        tgt = self.norm3(tgt + self.dropout4(tgt2))
        return tgt

# --- Matcher & losses (from your template, adapted for segmentation) ---
class HungarianMatcher(nn.Module):
    def __init__(self, cost_class: float = 2.0, cost_bbox: float = 5.0, cost_giou: float = 2.0, cost_mask: float = 5.0, cost_dice: float = 5.0):
        super().__init__()
        self.cost_class = cost_class
        self.cost_bbox = cost_bbox
        self.cost_giou = cost_giou
        self.cost_mask = cost_mask
        self.cost_dice = cost_dice

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.ops import box_convert, generalized_box_iou
from scipy.optimize import linear_sum_assignment

class HungarianMatcher(nn.Module):
    def __init__(self, cost_class: float = 2.0, cost_bbox: float = 5.0, cost_giou: float = 2.0, cost_mask: float = 5.0, cost_dice: float = 5.0):
        super().__init__()
        self.cost_class = cost_class
        self.cost_bbox = cost_bbox
        self.cost_giou = cost_giou
        self.cost_mask = cost_mask
        self.cost_dice = cost_dice

    @torch.no_grad()
    def forward(self, outputs, targets):
        bs, num_queries_per_image = outputs["pred_logits"].shape[:2]

        out_prob = outputs["pred_logits"].flatten(0, 1).softmax(-1)
        out_bbox = outputs["pred_boxes"].flatten(0, 1)
        out_mask = outputs["pred_masks"].flatten(0, 1)

        # --- FIX: Resize target masks to a consistent size before concatenation ---
        tgt_ids = torch.cat([v["labels"] for v in targets])
        tgt_bbox = torch.cat([v["boxes"] for v in targets])
        
        # 1. Determine the target spatial size from the model's output masks.
        target_size = out_mask.shape[-2:]
        
        # 2. Iterate, resize each mask tensor, and then concatenate.
        resized_tgt_masks = []
        for v in targets:
            masks = v["masks"]
            # Add a channel dimension if it's missing (N, H, W) -> (N, 1, H, W)
            if masks.dim() == 3:
                masks = masks.unsqueeze(1)
            
            resized_mask = F.interpolate(
                masks.float(), size=target_size, mode="bilinear", align_corners=False
            )
            # Remove the channel dimension after resizing
            resized_tgt_masks.append(resized_mask.squeeze(1))
            
        tgt_mask = torch.cat(resized_tgt_masks, dim=0)
        # --- End of Fix ---

        cost_class = -out_prob[:, tgt_ids]
        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)
        cost_giou = -generalized_box_iou(box_convert(out_bbox, "cxcywh", "xyxy"), box_convert(tgt_bbox, "cxcywh", "xyxy"))

        # The rest of the logic can now proceed with consistently sized masks.
        # Note: The original interpolation on tgt_mask is now redundant but harmless.
        # It's better to use the already-resized tgt_mask directly.
        out_mask_flat, tgt_mask_flat = out_mask.sigmoid().flatten(1), tgt_mask.flatten(1)

        # Calculate pairwise BCE mask cost
        num_preds = out_mask_flat.shape[0]
        num_targets = tgt_mask_flat.shape[0]
        out_mask_expanded = out_mask_flat.unsqueeze(1).expand(num_preds, num_targets, -1)
        tgt_mask_expanded = tgt_mask_flat.unsqueeze(0).expand(num_preds, num_targets, -1)
        # Use out_mask directly for logits version of BCE
        cost_mask = F.binary_cross_entropy(out_mask_expanded, tgt_mask_expanded, reduction="none").mean(-1)
        
        # Calculate Dice cost
        numerator = 2 * (out_mask_flat @ tgt_mask_flat.T)
        denominator = out_mask_flat.sum(-1)[:, None] + tgt_mask_flat.sum(-1)[None, :]
        cost_dice = 1 - (numerator + 1e-5) / (denominator + 1e-5)
        
        C = (self.cost_bbox * cost_bbox + self.cost_giou * cost_giou + self.cost_class * cost_class + self.cost_mask * cost_mask + self.cost_dice * cost_dice)

        # Correctly reshape cost matrix for distributed training
        C = C.view(bs, num_queries_per_image, -1).cpu()

        sizes = [len(v["boxes"]) for v in targets]
        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]
        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]

def dice_loss(inputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
    inputs = inputs.sigmoid().flatten(1)
    targets = targets.flatten(1)
    numerator = 2 * (inputs * targets).sum(1)
    denominator = inputs.sum(-1) + targets.sum(-1)
    return (1 - (numerator + 1e-5) / (denominator + 1e-5)).mean()

class SetCriterion(nn.Module):
    def __init__(self, num_classes: int, matcher: nn.Module, weight_dict: Dict[str, float], eos_coef: float = 0.1):
        super().__init__()
        self.num_classes = num_classes
        self.matcher = matcher
        self.weight_dict = weight_dict
        self.eos_coef = eos_coef
        empty_weight = torch.ones(self.num_classes + 1)
        empty_weight[-1] = self.eos_coef
        self.register_buffer("empty_weight", empty_weight)

    def _get_src_permutation_idx(self, indices):
        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])
        src_idx = torch.cat([src for (src, _) in indices])
        return batch_idx, src_idx

    def loss_labels(self, outputs, targets, indices, num_boxes):
        src_logits = outputs['pred_logits']
        idx = self._get_src_permutation_idx(indices)
        target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(targets, indices)])
        target_classes = torch.full(src_logits.shape[:2], self.num_classes, dtype=torch.int64, device=src_logits.device)
        target_classes[idx] = target_classes_o
        return {"loss_cls": F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)}

    def loss_boxes(self, outputs, targets, indices, num_boxes):
        idx = self._get_src_permutation_idx(indices)
        src_boxes = outputs['pred_boxes'][idx]
        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)
        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')
        loss_giou = 1 - torch.diag(generalized_box_iou(box_convert(src_boxes, "cxcywh", "xyxy"), box_convert(target_boxes, "cxcywh", "xyxy")))
        return {"loss_bbox": loss_bbox.sum() / num_boxes, "loss_giou": loss_giou.sum() / num_boxes}

    def loss_masks(self, outputs, targets, indices, num_boxes):
        src_idx = self._get_src_permutation_idx(indices)
        src_masks = outputs["pred_masks"][src_idx]
        target_masks = torch.cat([t['masks'][i] for t, (_, i) in zip(targets, indices)], dim=0)
        target_masks_resized = F.interpolate(target_masks.unsqueeze(1), size=src_masks.shape[-2:], mode="bilinear", align_corners=False).squeeze(1)
        loss_mask = F.binary_cross_entropy_with_logits(src_masks, target_masks_resized.to(src_masks.dtype), reduction='mean')
        loss_dice = dice_loss(src_masks, target_masks_resized.to(src_masks.dtype))
        return {"loss_mask": loss_mask, "loss_dice": loss_dice}

    def forward(self, outputs, targets):
        indices = self.matcher(outputs, targets)
        num_boxes = sum(len(t["labels"]) for t in targets)
        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)
        num_boxes = torch.clamp(num_boxes, min=1).item()
        losses = {}
        losses.update(self.loss_labels(outputs, targets, indices, num_boxes))
        losses.update(self.loss_boxes(outputs, targets, indices, num_boxes))
        losses.update(self.loss_masks(outputs, targets, indices, num_boxes))
        return {k: v * self.weight_dict[k] for k, v in losses.items() if k in self.weight_dict}

# --- Main Model: SparrowSegHead ---

class SparrowSegHead(BaseHead, nn.Module):
    """
    Main SparrowSegHead model.
    Implements the SparrowSeg architecture within the provided template structure.
    """
    def __init__(
        self,
        num_classes: int,
        backbone_type: str = "dino",
        image_size: int = 512,
        d_model: int = 256,
        num_queries: int = 100,
        num_decoder_layers: int = 2, # SparrowSeg uses a shallow decoder
        n_heads: int = 8,
        d_ffn: int = 1024,
        mask_dim: int = 256,
        **kwargs,
    ):
        BaseHead.__init__(self, num_classes, backbone_type, image_size, **kwargs)
        nn.Module.__init__(self)

        self.d_model = d_model
        self.num_queries = num_queries

        # 1. Backbone & Neck (using template's PANet wrapper)
        self.backbone = GenericBackboneWithFPN(
            backbone_type=backbone_type, fpn_out_channels=d_model, dummy_input_size=image_size
        )
        self.num_feature_levels = self.backbone.num_feature_levels

        # 2. Dynamic Query Generator
        self.query_generator = nn.Sequential(
            nn.Conv2d(d_model, d_model, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(d_model, num_queries * d_model)
        )

        # MLP to generate reference points from dynamic queries
        self.reference_point_head = nn.Linear(d_model, 4)

        # 3. Lightweight Deformable Decoder
        decoder_layer = DeformableTransformerDecoderLayer(d_model, n_heads, d_ffn, n_levels=self.num_feature_levels)
        self.transformer_decoder = nn.ModuleList(_get_clones(decoder_layer, num_decoder_layers))

        # 4. Prediction Heads
        self.class_embed = nn.Linear(d_model, num_classes + 1)
        self.bbox_embed = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, 4))
        self.mask_embed = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, mask_dim))
        self.pixel_proj = nn.Conv2d(d_model, mask_dim, kernel_size=1)

        self._init_criterion()

    def _init_criterion(self):
        matcher = HungarianMatcher(cost_class=2.0, cost_bbox=5.0, cost_giou=2.0, cost_mask=5.0, cost_dice=5.0)
        weight_dict = {"loss_cls": 2.0, "loss_bbox": 5.0, "loss_giou": 2.0, "loss_mask": 5.0, "loss_dice": 5.0}
        self.criterion = SetCriterion(self.num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=0.1)

    def forward(self, pixel_values: torch.Tensor, targets: Optional[List[Dict]] = None):
        device = pixel_values.device
        bs = pixel_values.shape[0]

        # 1. Get multi-scale features from backbone + neck
        feature_maps = self.backbone(pixel_values)
        srcs = list(feature_maps.values())
        
        # Identify high-res (for masks) and high-semantic (for queries) features
        high_res_feats = srcs[0]
        semantic_feats = srcs[-1]

        # 2. Generate dynamic queries
        instance_queries = self.query_generator(semantic_feats).view(bs, self.num_queries, self.d_model)

        # 3. Generate reference points for the dynamic queries
        reference_points = self.reference_point_head(instance_queries).sigmoid()

        # 4. Prepare features for Deformable Transformer
        memory_list, memory_spatial_shapes_list = [], []
        for src in srcs:
            h, w = src.shape[-2:]
            memory_list.append(src.flatten(2).transpose(1, 2))
            memory_spatial_shapes_list.append([h, w])
        memory = torch.cat(memory_list, dim=1)
        memory_spatial_shapes = torch.as_tensor(memory_spatial_shapes_list, dtype=torch.long, device=device)
        level_start_index = torch.cat((torch.zeros((1,), dtype=torch.long, device=device), memory_spatial_shapes.prod(1).cumsum(0)[:-1]))

        # 5. Pass through lightweight decoder
        decoder_output = instance_queries
        for layer in self.transformer_decoder:
            decoder_output = layer(
                tgt=decoder_output,
                memory=memory,
                ref_points_c=reference_points,
                memory_spatial_shapes=memory_spatial_shapes,
                level_start_index=level_start_index,
            )

        # 6. Prediction Heads
        logits = self.class_embed(decoder_output)
        pred_boxes_delta = self.bbox_embed(decoder_output)

        # Box decoding
        pred_center = reference_points[..., :2] + pred_boxes_delta[..., :2]
        pred_size = reference_points[..., 2:] * pred_boxes_delta[..., 2:].exp()
        pred_boxes = torch.cat([pred_center, pred_size], dim=-1).sigmoid()

        # Mask decoding
        mask_embeds = self.mask_embed(decoder_output)
        pixel_feats = self.pixel_proj(high_res_feats)
        pred_masks = torch.einsum("bqc,bchw->bqhw", mask_embeds, pixel_feats)

        outputs = {"pred_logits": logits, "pred_boxes": pred_boxes, "pred_masks": pred_masks}

        losses = {}
        if targets is not None:
            # The criterion calculates all the losses and returns a dictionary
            losses = self.criterion(outputs, targets)

        # Always return both outputs and the (potentially empty) losses dictionary
        return outputs, losses

# --- Example Usage ---
if __name__ == '__main__':
    print("ðŸš€ Initializing SparrowSegHead...")

    # Model configuration
    model = SparrowSegHead(
        num_classes=80,
        backbone_type="timm-convnextv2_atto.fcmae",
        image_size=512,
        d_model=256,
        num_queries=100,
        num_decoder_layers=2, # Key efficiency parameter
    ).eval()

    print(f"\nâœ… SparrowSegHead created successfully!")
    print(f"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M")

    # Create a dummy input batch
    dummy_image = torch.randn(2, 3, 512, 512)

    # --- Inference Example ---
    print("\n--- Running Inference ---")
    with torch.no_grad():
        outputs = model(dummy_image)

    print("Output shapes:")
    for key, value in outputs.items():
        print(f"  - {key}: {value.shape}")

    # --- Training Example ---
    print("\n--- Running Training Step ---")
    model.train()

    # Create dummy targets for training
    dummy_targets = [
        {"labels": torch.tensor([10, 20]), "boxes": torch.rand(2, 4).sigmoid(), "masks": torch.randint(0, 2, (2, 512, 512)).float()},
        {"labels": torch.tensor([30]), "boxes": torch.rand(1, 4).sigmoid(), "masks": torch.randint(0, 2, (1, 512, 512)).float()}
    ]

    outputs, losses = model(dummy_image, dummy_targets)

    print("Losses calculated:")
    total_loss = 0
    for key, value in losses.items():
        print(f"  - {key}: {value.item():.4f}")
        total_loss += value

    print(f"\nTotal weighted loss: {total_loss.item():.4f}")
    # In a real training loop, you would call total_loss.backward() here.
